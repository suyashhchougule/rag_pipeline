# ===================
# PLATFORM SETTINGS
# ===================
# The main platform to use for inference ("CEREBRAS" or others you might add)
PLATFORM = "CEREBRAS"

# ===================
# CEREBRAS SETTINGS
# ===================
[CEREBRAS_LLAMA4]
# The model identifier for Cerebras API (can be swapped with other supported model IDs)
MODEL_ID = 'llama-4-scout-17b-16e-instruct'
#API_KEY variable reference in .secrets.toml
API_KEY = "CEREBRAS_KEY"
# Cerebras API endpoint (You can use any OpenAI-compatible endpoint here)
ENDPOINT = "https://api.cerebras.ai/v1"
# Sampling temperature for generation (lower = more deterministic, higher = more random)
TEMPERATURE = 0.1
# Maximum number of tokens to generate in a single response
MAX_TOKENS = 8192

[CEREBRAS_QWEN]
MODEL_ID = 'qwen-3-235b-a22b-thinking-2507'
API_KEY = "CEREBRAS_KEY"
ENDPOINT = "https://api.cerebras.ai/v1"
TEMPERATURE = 0.1
MAX_TOKENS = 65536

[AGENT]
MODEL_ID = 'gpt-4o'
API_KEY = "YOUR_API_KEY"
ENDPOINT = "OPENAI Compatible Endpoint"
TEMPERATURE = 0.01
MAX_TOKENS = 8192

#Add any other model config, if required

[DEFAULT]
# Default model for generation (Please change this if you want to use ANY other above-listed model config)
GENERATOR_MODEL_SECTION = "AGENT"

# ===================
# INGESTION SETTINGS
# ===================
[INGESTION]
# SQLite database file to track which files have been ingested
TRACKER_DB = "ingestion_pipeline/ingestion_databases/ingested_files.db"
# Database for parent document relationships
PARENTS_DB = "ingestion_pipeline/ingestion_databases/parents.db"
# Directory for storing all index files
INDEX_DIR = "ingestion_pipeline/ingestion_databases/indexes"
# Directory for sentence-level indexes
SENT_IDX_DIR = "ingestion_pipeline/ingestion_databases/indexes/sentences"
# Directory for parent document indexes
PARENT_IDX_DIR = "ingestion_pipeline/ingestion_databases/indexes/parents"
# Name or path of the embedding model (e.g., any model from Hugging Face)
EMBED_MODEL = "Qwen/Qwen3-Embedding-0.6B"
# Local path to your Dolphin model for document text and structure extraction 
DOLPHIN_MODEL_PATH = "./hf_model/"
# Number of documents to process in each batch during parsing
DOCUMENT_PARSER_BATCH_SIZE = 256
# Where to save intermediate outputs from the document parser
DOCUMENT_PARSER_SAVE_DIR = "ingestion_pipeline/document_parser_save_dir"

RECHUNK = true

# ===================
# RATE LIMITER SETTINGS
# ===================
[RATELIMITER]
# Requests per minute allowed for your endpoint (RPM)
RPM = 30
# Tokens per minute allowed for your endpoint(TPM)
TPM = 60000

# ===================
# LOGGER SETTINGS
# ===================
[LOGGER]
# Path to the API log file
API_FILE_PATH = "../rag_pipeline_api.log"
