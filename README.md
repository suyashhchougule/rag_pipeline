# RAG Pipeline ğŸ“šğŸ”

A **modular retrieval-augmented generation (RAG)** pipeline that ingests documents (PDF, images), builds vector indexes using FAISS, and exposes a FastAPI endpoint for querying via a chat-based LLM.

---

## ğŸš€ Features

* ğŸ”„ **Modular ingestion pipeline**

  * Converts PDF or image documents to Markdown using ByteDance Dolphin (Vision-Language Model)
  * Normalizes HTML tables, splits into sections, chunks into parent and sentence-level documents
  * Skips re-ingestion of unchanged files using SHA-256 file tracking
  * Stores parent chunks in SQLite and maintains FAISS indexes for retrieval
* ğŸ¤– **Retrieval and generation**

  * FastAPI `/query` endpoint that retrieves context and calls LLM for structured answers
  * Supports rate-limiting and token logging for OpenAI-compatible LLMs
* ğŸ“¦ **DevOps-ready**

  * Fully containerizable via Docker
  * Configurable via TOML (`dynaconf`)
  * Logging to file and console with a structured format

---

## ğŸ“ Repository Structure

```text
â”œâ”€â”€ README.md                                   # Project overview and setup
â”œâ”€â”€ app                                         # API & core runtime
â”‚   â”œâ”€â”€ api.py                                  # FastAPI entryâ€‘point
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â””â”€â”€ generator.py                        # Generic abstraction class for LLM API Calls
â”‚   â”œâ”€â”€ docstore.py                             # SQLite doc blob store
â”‚   â”œâ”€â”€ generator.py                            # (wrapper for generator)
â”‚   â”œâ”€â”€ index_loader.py                         # Load FAISS indexes
â”‚   â”œâ”€â”€ main.py                                 # CLI/demo
â”‚   â”œâ”€â”€ prompt_templates.py                     # Prompt templates for RAG/LLM
â”‚   â”œâ”€â”€ ratelimiter.py                          # Request/token limiter
â”‚   â”œâ”€â”€ retriever.py                            # Multi-level retriever logic
â”‚   â””â”€â”€ tokenlogger.py                          # Token usage tracking
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ RagConfig.toml                          # Main settings (paths, models)
â”‚   â””â”€â”€ .secrtets.toml                          # Secrets (API keys, tokens)
â”œâ”€â”€ docs/                                       # Sample docs to ingest
â”œâ”€â”€ hf_model/                                   # Local Dolphin VLM files
â”œâ”€â”€ ingestion_pipeline/                         # Ingestion & chunking
â”‚   â”œâ”€â”€ chunker.py
â”‚   â”œâ”€â”€ document_parser_save_dir/               # Saved markdown & images
â”‚   â”œâ”€â”€ documentparser.py
â”‚   â”œâ”€â”€ filetracker.py
â”‚   â”œâ”€â”€ indexer.py
â”‚   â”œâ”€â”€ ingest.py
â”‚   â”œâ”€â”€ ingestion_databases/                    # SQLite + FAISS data
â”‚   â”œâ”€â”€ loader.py
â”‚   â”œâ”€â”€ store.py
â”‚   â””â”€â”€ utils/
â”œâ”€â”€ pyproject.toml                              # Build & dependency file
â”œâ”€â”€ Dockerfile                                  # Docker build recipe
```

---

## ğŸ› ï¸ Getting Started

### Prerequisites

* Python 3.10+
* Docker (optional)
* CUDA (for GPU acceleration)

### 1. Clone & Install

Quick setup with [uv](https://github.com/astral-sh/uv) package manager:

```bash
# Install uv globally (oneâ€‘time)
pip install uv

# Clone repo
git clone https://github.com/suyashhchougule/rag_pipeline.git
cd rag_pipeline

# Create env & install deps in one go
uv venv
source .venv/bin/activate
uv sync
```

---

### 2. Configure

1. **Create your Cerebras API key**  
   - Go to the [Cerebras Cloud platform](https://cloud.cerebras.ai/platform/org_4myhnjpmnjyx3mrjt3nwmy33/apikeys):  
   - Log in (or sign up if you donâ€™t have an account).  
   - Click **â€œCreate API Keyâ€**, copy the generated key.

2. **Populate your secrets file**  
   ```bash
   config/.secrets.toml
3. Configure `config/RagConfig.toml`, if required. 

### 3. Run Ingestion

```bash
python ingestion_pipeline/ingest.py --input_folder ./docs
```

This will parse your files, chunk them, build/update vector indexes, and store everything.

### 4. Launch the API

```bash
cd app
uvicorn app.api:app --reload --port 8000
```

Then query via HTTP:

```bash
curl -X POST localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{ "question": "Define affinity fraud" }'
```
###  Architecture Overview: 

## ğŸ“¥ Ingestion & Document Understanding

Uses **[ByteDance Dolphin](https://huggingface.co/ByteDance/Dolphin)** ([GitHub](https://github.com/bytedance/Dolphin)), a state-of-the-art visionâ€“language model that converts both **scanned** *and* **digital PDFs** into richly structured Markdown including headings, sections, paragraphs, tables, figures.

* **Why Dolphin?**  
  *Heterogeneous-Anchor Prompting* lets the model detect layout first, then extract text, giving higher fidelity than single-pass VLMs.  
* **Benefits for RAG**  
  - Eliminates the OCR vs. digital PDF fork.  
  - Emits clean block-level structure, which downstream chunkers can trust.  
  - Handles noisy scans without template tuning.

The extractor runs once per document during ingestion; unchanged files are skipped via SHA-256 digests, keeping re-ingestion idempotent.

**Alternatives evaluated**

|  | 
|-----------|
| **docling** â€” <https://github.com/docling-project/docling> | 
| **MinerU** â€” <https://github.com/opendatalab/MinerU> |


---


## ğŸ”ª Chunking Strategy

We combine **hierarchical** *and* **structure-aware** chunking because no single split size works for all queries.

| Level            | Method                     | Size / Overlap          | Purpose                        |
|------------------|----------------------------|-------------------------|--------------------------------|
| **Sentence**     | Sentence splitter          | â‰ˆ 1â€“2 sentences, 0 ovl  | Precise keyword hits           |
| **Intermediate** | Recursive splitter         | **512 tokens**, 64 ovl  | Balance locality & context     |
| **Parent**       | Recursive splitter         | **2048 tokens**, 128 ovl| Global coherence               |
| **Markdown**     | Header-aware splitter      | `<h1â€¦h6>` boundaries    | Preserve author organisation   |

1. **Hierarchical Re-ranker Retriever (HRR)**  
   HRR reranks intermediate-size chunks against the query, then surfaces their parent context, lifting recall without flooding the prompt.  
   *Reference:* â€œHierarchical Retrieval for RAGâ€ ([arXiv 2503.02401](https://arxiv.org/pdf/2503.02401)).

2. **Document-Structure-Aware Chunking**  
   When Markdown headings exist, we split on those natural boundaries instead of fixed windows, boosting retrieval accuracy by **5â€“10 %** in finance & tech docs.  
   *References:*  
   â€¢ [Snowflake Engineering â€“ Impact of Retrieval Chunking in Finance RAG](https://www.snowflake.com/en/engineering-blog/impact-retrieval-chunking-finance-rag/)  
   â€¢ [Milvus â€“ What Chunking Strategies Work Best?](https://milvus.io/ai-quick-reference/what-chunking-strategies-work-best-for-document-indexing)

> **Trade-off:** Hierarchical splitting increases index size, but HRR avoids extra LLM calls, so latency stays < 500 ms. Structure-aware splits fall back to recursive rules when documents lack headings, ensuring robustness.

## ğŸ§¬ Embeddings & Vector Index

### Embedding Model  

Embeds both queries and chunks with **[Qwen 3 Embedding 0.6 B](https://huggingface.co/Qwen/Qwen3-Embedding-0.6B)** â€” a 600 M-parameter model that currently sits **4áµ—Ê° on the [MTEB leaderboard](<https://huggingface.co/spaces/mteb/leaderboard>)**.  
The model offers near-state-of-the-art semantic retrieval quality while remaining lightweight enough to run on CPU or a single consumer GPU, making it a pragmatic â€œminimal yet robustâ€ choice.


### FAISS Index  

Uses **FAISS** for its zero-friction setup and tight local latency.  
Because the retriever is abstracted behind a thin `VectorStore` interface, FAISS can be swapped for a production-grade service (Azure Search, OpenSearch, etc.).

**Hierarchical Re-ranker support**  
Maintains **two indexes**:  

| Index | Granularity | Purpose |
|-------|-------------|---------|
| `faiss_sentence.index` | **Sentence-level embeddings** | High-precision keyword hits and their corresponding parent retrieval|
| `faiss_parent.index`   | **Parent (â‰ˆ2 048-token) embeddings** | Broader context for answer grounding |

This dual-index layout powers the **Hierarchical Re-ranker Retriever (HRR)**:  

## ğŸ” Retrieval Strategy (Multi-Level)

Uses a **two-index, multi-level retrieval flow** inspired by HRR:

1. **Parallel search**  
   * Query the **sentence index** for the top-**15** hits (`k_sentence = 15`).  
   * Query the **parent index** for the top-**15** hits (`k_parent = 15`).

2. **Parent mapping & merge**  
   * Each sentence-level hit is mapped to its `parent_id`.  
   * Direct parent hits keep their own `uid`.  
   * We retain the **single best similarity score** per parent.

3. **Score sort & cut-off**  
   * Merge the two parent lists.  
   * Sort descending by cosine similarity.  
   * Return the first **`max_return` parent chunks** (configurable, default **5**).

4. **Context assembly**  
   * Load the selected parent chunks from the SQLite doc-store.  
   * Pass them â€” plus their similarity scores â€” to the prompt builder.

> **Why this works**  
> â€¢ Sentence search captures fine-grained matches; parent search surfaces broader context.  
> â€¢ De-duplication keeps one score per parent, preventing prompt spam.  

## ğŸ“ LLM & Prompt Design

### Generator Abstraction  
`app/generator.py` is a thin wrapper that can call **any OpenAI-compatible endpoint** by reading `endpoint_url` and `api_key` from the TOML config. Swap in Azure OpenAI, Google Vertex, Hugging Face Inference Endpoints, vLLM, or a self-hosted model without touching application code.

| Default model (demo) | Provider | Params | Endpoint |
|----------------------|----------|--------|----------|
| `llama-4-scout-17b-16e-instruct` | Cerebras | 17 B | `https://api.cerebras.ai/v1` |

### Rate Limiting  
A class-level **RateLimiter** enforces requests-per-minute *and* tokens-per-minute ceilings.  
Benefits:  
* Prevents hard 429s / quota bans.  
* Smooths traffic spikes (protects SLAs).  
* Caps runaway cost in pay-as-you-go billing.  
Values are configurable from config.

### JSON-Mode Support  
When the upstream model advertises â€œ`mode: json`â€, the generator sets `response_format={"type":"json_object"}`.  
*Guarantees* syntactically valid JSON, easing downstream parsing and evals.

### Prompt Template (COSTAR-inspired)
Uses **[COSTAR Prompt Technique](https://medium.com/@frugalzentennial/unlocking-the-power-of-costar-prompt-engineering-a-guide-and-example-on-converting-goals-into-dc5751ce9875)**
```text
## Persona
You are a helpful assistant trusted for accurate, reference-backed answers.

## Instruction
1. Answer the question **only** with facts you can locate in <context>.  
2. If the context does **not** contain the answer, reply exactly with â€œInsufficient context".  
3. Use complete sentences and provide enough detail for clarity.  
4. Do not invent or add information that is not present in the context.

## Context
<context>
{context}
</context>

## Question
{question}

## Tone
Adopt a concise and helpful tone, informative yet approachable.

## Audience
Respond to audience who may not share your background knowledge; keep jargon minimal.

## Output Format
Return a valid JSON object with these keys **and no additional keys**:

json
{{
  "answer": "<your best answer or 'â€œInsufficient context'>",
}}
```

## ğŸ”Œ API Reference

### `POST /query`

#### Request body
| Field      | Type | Default | Description |
|------------|------|---------|-------------|
| `question` | `str` | â€” (required) | Natural-language query |

> Send `Content-Type: application/json`.

#### Successful response `200`
```jsonc
{
  "answer": {                     // JSON answer from the LLM
    "answer": "Affinity fraud is â€¦"
  },
  "context_chunks": [             // Evidence passed to the LLM
    {
      "chunk_id": "chunk uid",
      "summary": "â€¦text of the chunkâ€¦",
      "metadata": { "page": 7 },
      "score": 0.84
    }
  ],
  "meta": {                       // Trace metadata
    "embedding_model": "Qwen3-Embedding-0.6B",
    "generator_model": "llama-4-scout-17b-16e-instruct",
    "retriever": "MultiLevelRetriever",
    "prompt_template": "PROMPT_TMPL",
    "num_context_chunks": 3
  }
}
```

| Status | Condition                     | Body example                                 |
| ------ | ----------------------------- | -------------------------------------------- |
| `404`  | No relevant context found     | `{ "detail": "No relevant context found." }` |
| `500`  | Retriever / prompt / LLM fail | `{ "detail": "Generator error: â€¦" }`         |

## ğŸ“ˆ Observability & Logging

| Aspect | Implementation |
|--------|----------------|
| **Library / format** | Python `logging` with a **structured plain-text formatter**<br>`"%(asctime)s %(levelname)s %(name)s %(message)s"` |
| **Per-request fields** | â€¢ timestamp â€¢ level â€¢ logger name â€¢ query text â€¢ retrieved-chunk count â€¢ token in/out â€¢ latency ms â€¢ exception detail |
| **Sinks** | 1. **stdout** (for Docker/â€†Kubernetes log scraping)<br>2. Rotating files: `rag_pipeline.log` (ingestion/runtime) and `rag_pipeline_api.log` (FastAPI endpoint) |
```
Example entry:
2025-07-17 15:21:20,635 INFO generator setting generation model (llama-4-scout-17b-16e-instruct)
2025-07-17 15:21:27,453 INFO api Received query: What are common investment scams on  social media?
2025-07-17 15:21:28,110 INFO api Retrieved 10 parent chunks for query.
2025-07-17 15:19:09,809 INFO httpx HTTP Request: POST https://api.cerebras.ai/v1/chat/completions "HTTP/1.1 200 OK"
2025-07-17 15:19:09,859 INFO generator Model response (CEREBRAS_LLAMA4) in 0.96s
2025-07-17 15:19:09,859 INFO tokenlogger Token usage - prompt: 2097, completion: 126
2025-07-17 15:19:09,859 INFO api LLM generation complete for query.
```
---
## â¤ï¸ Acknowledgements

Powered by:

* ByteDance **Dolphin** (VLM)
* FAISS
* FastAPI
* Opens source **LLaMA**â€“like models
* Docker

---
