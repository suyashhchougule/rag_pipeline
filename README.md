# RAG Pipeline ğŸ“šğŸ”

A **modular retrieval-augmented generation (RAG)** pipeline that ingests documents (PDF, images), builds vector indexes using FAISS, and exposes a FastAPI endpoint for querying via a chat-based LLM.

---

## ğŸš€ Features

* ğŸ”„ **Modular ingestion pipeline**

  * Converts PDF or image documents to Markdown using ByteDance Dolphin (Vision-Language Model)
  * Normalizes HTML tables, splits into sections, chunks into parent and sentence-level documents
  * Skips re-ingestion of unchanged files using SHA-256 file tracking
  * Stores parent chunks in SQLite and maintains FAISS indexes for retrieval
* ğŸ¤– **Retrieval and generation**

  * FastAPI `/query` endpoint that retrieves context and calls LLM for structured answers
  * Supports rate-limiting and token logging for OpenAI-compatible LLMs
* ğŸ“¦ **DevOps-ready**

  * Fully containerizable via Docker
  * Configurable via TOML (`dynaconf`)
  * Logging to file and console with a structured format

---

## ğŸ“ Repository Structure

```text
â”œâ”€â”€ README.md                                   # Project overview and setup
â”œâ”€â”€ app                                         # API & core runtime
â”‚   â”œâ”€â”€ api.py                                  # FastAPI entryâ€‘point
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â””â”€â”€ generator.py                        # Generic abstraction class for LLM API Calls
â”‚   â”œâ”€â”€ docstore.py                             # SQLite doc blob store
â”‚   â”œâ”€â”€ generator.py                            # (wrapper for generator)
â”‚   â”œâ”€â”€ index_loader.py                         # Load FAISS indexes
â”‚   â”œâ”€â”€ main.py                                 # CLI/demo
â”‚   â”œâ”€â”€ prompt_templates.py                     # Prompt templates for RAG/LLM
â”‚   â”œâ”€â”€ ratelimiter.py                          # Request/token limiter
â”‚   â”œâ”€â”€ retriever.py                            # Multi-level retriever logic
â”‚   â””â”€â”€ tokenlogger.py                          # Token usage tracking
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ RagConfig.toml                          # Main settings (paths, models)
â”‚   â””â”€â”€ .secrtets.toml                          # Secrets (API keys, tokens)
â”œâ”€â”€ docs/                                       # Sample docs to ingest
â”œâ”€â”€ hf_model/                                   # Local Dolphin VLM files
â”œâ”€â”€ ingestion_pipeline/                         # Ingestion & chunking
â”‚   â”œâ”€â”€ chunker.py
â”‚   â”œâ”€â”€ document_parser_save_dir/               # Saved markdown & images
â”‚   â”œâ”€â”€ documentparser.py
â”‚   â”œâ”€â”€ filetracker.py
â”‚   â”œâ”€â”€ indexer.py
â”‚   â”œâ”€â”€ ingest.py
â”‚   â”œâ”€â”€ ingestion_databases/                    # SQLite + FAISS data
â”‚   â”œâ”€â”€ loader.py
â”‚   â”œâ”€â”€ store.py
â”‚   â””â”€â”€ utils/
â”œâ”€â”€ pyproject.toml                              # Build & dependency file
â”œâ”€â”€ Dockerfile                                  # Docker build recipe
```

---

## ğŸ› ï¸ Getting Started

### Prerequisites

* Python 3.10+
* Docker (optional)
* CUDA (for GPU acceleration)

### 1. Clone & Install

Quick setup with [uv](https://github.com/astral-sh/uv) package manager:

```bash
# Install uv globally (oneâ€‘time)
pip install uv

# Clone repo
git clone https://github.com/suyashhchougule/rag_pipeline.git
cd rag_pipeline

# Create env & install deps in one go
uv venv
source .venv/bin/activate
uv sync
```

---

### 2. Configure

1. **Create your Cerebras API key**  
   - Go to the [Cerebras Cloud platform](https://cloud.cerebras.ai/platform/org_4myhnjpmnjyx3mrjt3nwmy33/apikeys):  
   - Log in (or sign up if you donâ€™t have an account).  
   - Click **â€œCreate API Keyâ€**, copy the generated key.

2. **Populate your secrets file**  
   ```bash
   config/.secrets.toml
3. Configure config/RagConfig.toml, if required. 

### 3. Run Ingestion

```bash
python ingestion_pipeline/ingest.py --input_folder ./docs
```

This will parse your files, chunk them, build/update vector indexes, and store everything.

### 4. Launch the API

```bash
cd app
uvicorn app.api:app --reload --port 8000
```

Then query via HTTP:

```bash
curl -X POST localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{ "question": "Define affinity fraud" }'
```

---

## ğŸ³ Docker Setup

### Build image:

```bash
docker build -t rag-api .
```

### Run container (Option A: built-in model download):

```bash
docker run -p 8000:8000 rag-api
```
---
## â¤ï¸ Acknowledgements

Powered by:

* ByteDance **Dolphin** (VLM)
* FAISS
* FastAPI
* Opens AI-compatible **LLaMA**â€“like models
* Docker

---
